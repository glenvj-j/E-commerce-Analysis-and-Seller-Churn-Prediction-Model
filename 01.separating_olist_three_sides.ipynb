{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Loading Datasets\n",
    "\n",
    "To kick off the analysis, we start by importing the necessary libraries. The `pandas` library is our go-to tool for data manipulation, and we set some display options to ensure we can see all columns and a limited number of rows for better readability. Next, we load all the relevant datasets, including customer information, geolocation data, order details, product information, seller data, reviews, payments, and even a translation for product categories. These datasets will serve as the foundation for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# Define dataset folder path (works for both Windows & Mac)\n",
    "dataset_folder = os.path.join(\"dataset\", \"00_original_dataset\")\n",
    "\n",
    "# Load datasets using os.path.join()\n",
    "customers = pd.read_csv(os.path.join(dataset_folder, \"olist_customers_dataset.csv\"))\n",
    "geolocation = pd.read_csv(os.path.join(dataset_folder, \"olist_geolocation_dataset.csv\"))\n",
    "order_items = pd.read_csv(os.path.join(dataset_folder, \"olist_order_items_dataset.csv\"))\n",
    "orders = pd.read_csv(os.path.join(dataset_folder, \"olist_orders_dataset.csv\"))\n",
    "products = pd.read_csv(os.path.join(dataset_folder, \"olist_products_dataset.csv\"))\n",
    "sellers = pd.read_csv(os.path.join(dataset_folder, \"olist_sellers_dataset.csv\"))\n",
    "order_reviews = pd.read_csv(os.path.join(dataset_folder, \"olist_order_reviews_dataset.csv\"))\n",
    "order_payments = pd.read_csv(os.path.join(dataset_folder, \"olist_order_payments_dataset.csv\"))\n",
    "category_translation = pd.read_csv(os.path.join(dataset_folder, \"product_category_name_translation.csv\"))\n",
    "\n",
    "print(\"Datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Missing ZIP code\n",
    "\n",
    "We will check if all the ZIP in seller and customer are already in geolocation, if not we will try to fill that with nearest long and lat area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Missing Customer Zip Codes: 157 found\n",
      "[28160, 56327, 75784, 29196, 71698, 68629, 29718, 70686, 67105, 73255, 71208, 12332, 70701, 70702, 72238, 72237, 7729, 72242, 72243, 64047]\n",
      "\n",
      "üîç Missing Seller Zip Codes: 7 found\n",
      "{72580, 37708, 2285, 7412, 82040, 91901, 71551}\n"
     ]
    }
   ],
   "source": [
    "# Get unique zip codes\n",
    "customer_zip_unique = set(customers[\"customer_zip_code_prefix\"].unique())\n",
    "seller_zip_unique = set(sellers[\"seller_zip_code_prefix\"].unique())\n",
    "geo_zip_unique = set(geolocation[\"geolocation_zip_code_prefix\"].unique())\n",
    "\n",
    "# Find missing zip codes \n",
    "missing_customer_zip = customer_zip_unique - geo_zip_unique\n",
    "missing_seller_zip = seller_zip_unique - geo_zip_unique\n",
    "\n",
    "# Print results\n",
    "print(f\"üîç Missing Customer Zip Codes: {len(missing_customer_zip)} found\")\n",
    "print(missing_customer_zip if len(missing_customer_zip) < 20 else list(missing_customer_zip)[:20])\n",
    "\n",
    "print(f\"\\nüîç Missing Seller Zip Codes: {len(missing_seller_zip)} found\")\n",
    "print(missing_seller_zip if len(missing_seller_zip) < 20 else list(missing_seller_zip)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Missing zip codes assigned and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from geopy.distance import great_circle\n",
    "# Keep only unique zip codes from geolocation\n",
    "geo_unique = geolocation.drop_duplicates(subset=\"geolocation_zip_code_prefix\")\n",
    "\n",
    "# Function to find the nearest available zip code\n",
    "def find_nearest_zip(missing_zip, geo_df):\n",
    "    min_dist = float('inf')\n",
    "    nearest_lat, nearest_lng = None, None\n",
    "\n",
    "    for _, row in geo_df.iterrows():\n",
    "        lat, lng = row[\"geolocation_lat\"], row[\"geolocation_lng\"]\n",
    "        \n",
    "        # Calculate distance using Haversine formula\n",
    "        dist = great_circle((lat, lng), (0, 0)).km  # Approximate reference point\n",
    "        \n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            nearest_lat, nearest_lng = lat, lng\n",
    "\n",
    "    return nearest_lat, nearest_lng\n",
    "\n",
    "# Find missing customer zip codes\n",
    "missing_customer_zips = set(customers[\"customer_zip_code_prefix\"].unique()) - set(geo_unique[\"geolocation_zip_code_prefix\"].unique())\n",
    "\n",
    "# Create a DataFrame for missing zip codes\n",
    "missing_customer_geo = pd.DataFrame(\n",
    "    [(zip_code, *find_nearest_zip(zip_code, geo_unique)) for zip_code in missing_customer_zips],\n",
    "    columns=[\"geolocation_zip_code_prefix\", \"geolocation_lat\", \"geolocation_lng\"]\n",
    ")\n",
    "\n",
    "# Find missing seller zip codes\n",
    "missing_seller_zips = set(sellers[\"seller_zip_code_prefix\"].unique()) - set(geo_unique[\"geolocation_zip_code_prefix\"].unique())\n",
    "\n",
    "# Create a DataFrame for missing seller zip codes\n",
    "missing_seller_geo = pd.DataFrame(\n",
    "    [(zip_code, *find_nearest_zip(zip_code, geo_unique)) for zip_code in missing_seller_zips],\n",
    "    columns=[\"geolocation_zip_code_prefix\", \"geolocation_lat\", \"geolocation_lng\"]\n",
    ")\n",
    "\n",
    "# Concatenate with the original geolocation dataset\n",
    "geolocation_fixed = pd.concat([geolocation, missing_customer_geo, missing_seller_geo], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "folder_path = \"dataset/00_geoloc_data\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Define file paths\n",
    "geoloc_dataset_path = os.path.join(folder_path, \"Fixed_Geolocation.csv\")\n",
    "\n",
    "# Save DataFrames to CSV\n",
    "geolocation_fixed.to_csv(geoloc_dataset_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Missing zip codes assigned and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = pd.read_csv(\"dataset/00_geoloc_data/Fixed_Geolocation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geolocation_zip_code_prefix</th>\n",
       "      <th>geolocation_lat</th>\n",
       "      <th>geolocation_lng</th>\n",
       "      <th>geolocation_city</th>\n",
       "      <th>geolocation_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000261</th>\n",
       "      <td>38710</td>\n",
       "      <td>28.008978</td>\n",
       "      <td>-15.536867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         geolocation_zip_code_prefix  geolocation_lat  geolocation_lng  \\\n",
       "1000261                        38710        28.008978       -15.536867   \n",
       "\n",
       "        geolocation_city geolocation_state  \n",
       "1000261              NaN               NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo[geo['geolocation_zip_code_prefix']==38710]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/mp18g7bj3fs8hm27kth1hcvw0000gn/T/ipykernel_23298/2773666176.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  geolocation_filled[\"geolocation_city\"].fillna(geolocation_filled[\"geolocation_city_new\"], inplace=True)\n",
      "/var/folders/dt/mp18g7bj3fs8hm27kth1hcvw0000gn/T/ipykernel_23298/2773666176.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  geolocation_filled[\"geolocation_state\"].fillna(geolocation_filled[\"geolocation_state_new\"], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Missing city & state values have been filled using customers & sellers data!\n"
     ]
    }
   ],
   "source": [
    "geolocation_fixed = pd.read_csv(\"dataset/00_geoloc_data/Fixed_Geolocation.csv\")\n",
    "\n",
    "# Load city & state mappings from customers and sellers\n",
    "customer_zip_mapping = customers.groupby(\"customer_zip_code_prefix\").agg({\"customer_city\": \"first\", \"customer_state\": \"first\"})\n",
    "seller_zip_mapping = sellers.groupby(\"seller_zip_code_prefix\").agg({\"seller_city\": \"first\", \"seller_state\": \"first\"})\n",
    "\n",
    "# Rename columns for merging\n",
    "customer_zip_mapping = customer_zip_mapping.rename(columns={\"customer_city\": \"geolocation_city\", \"customer_state\": \"geolocation_state\"})\n",
    "seller_zip_mapping = seller_zip_mapping.rename(columns={\"seller_city\": \"geolocation_city\", \"seller_state\": \"geolocation_state\"})\n",
    "\n",
    "# Combine both mappings (customers & sellers)\n",
    "zip_mapping = pd.concat([customer_zip_mapping, seller_zip_mapping]).reset_index().drop_duplicates(subset=\"index\")\n",
    "zip_mapping = zip_mapping.rename(columns={\"index\": \"geolocation_zip_code_prefix\"})\n",
    "\n",
    "# Merge the mapping into the geolocation dataset\n",
    "geolocation_filled = geolocation_fixed.merge(zip_mapping, on=\"geolocation_zip_code_prefix\", how=\"left\", suffixes=(\"\", \"_new\"))\n",
    "\n",
    "# Fill missing values from the mapping\n",
    "geolocation_filled[\"geolocation_city\"].fillna(geolocation_filled[\"geolocation_city_new\"], inplace=True)\n",
    "geolocation_filled[\"geolocation_state\"].fillna(geolocation_filled[\"geolocation_state_new\"], inplace=True)\n",
    "\n",
    "# Drop helper columns\n",
    "geolocation_filled = geolocation_filled.drop(columns=[\"geolocation_city_new\", \"geolocation_state_new\"])\n",
    "\n",
    "\n",
    "# Define dataset folder path (cross-platform)\n",
    "folder_path = os.path.join(\"dataset\", \"00_geoloc_data\")\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Define file paths using os.path.join()\n",
    "geoloc_dataset_path = os.path.join(folder_path, \"Final_Geolocation.csv\")\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "geolocation_filled.to_csv(geoloc_dataset_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Missing city & state values have been filled using customers & sellers data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geolocation_zip_code_prefix</th>\n",
       "      <th>geolocation_lat</th>\n",
       "      <th>geolocation_lng</th>\n",
       "      <th>geolocation_city</th>\n",
       "      <th>geolocation_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [geolocation_zip_code_prefix, geolocation_lat, geolocation_lng, geolocation_city, geolocation_state]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geolocation[geolocation['geolocation_zip_code_prefix']==38710]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation = geolocation_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Geolocation Data\n",
    "\n",
    "The geolocation dataset contains detailed information about zip codes, cities, states, and their corresponding latitude and longitude. To simplify this data, we group it by the `geolocation_zip_code_prefix` column. For each unique zip code prefix, we calculate the average latitude and longitude and take the first occurrence of the city and state. This grouped data will later help us merge geolocation information with other datasets based on zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_grouped = geolocation.groupby(\"geolocation_zip_code_prefix\").agg({\n",
    "    \"geolocation_lat\": \"mean\",\n",
    "    \"geolocation_lng\": \"mean\",\n",
    "    \"geolocation_city\": \"first\",\n",
    "    \"geolocation_state\": \"first\"\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Order Reviews and Payments\n",
    "\n",
    "In this step, we focus on simplifying the order reviews and payments data. For each order, there might be multiple reviews or payments, so we group the data by `order_id` and calculate the **median** `review_score` and `payment_value` for each order. This reduces the complexity of the data and ensures that each order is represented by a single review score and payment value, making it easier to analyze. By using the median, we mitigate the impact of any outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_reviews = (\n",
    "    order_reviews.groupby('order_id')['review_score']\n",
    "    .median()\n",
    "    .apply(np.floor)\n",
    "    .reset_index()\n",
    ")\n",
    "order_payments = order_payments.groupby('order_id').agg({\n",
    "    # 'payment_value': 'median',  \n",
    "    'payment_type': lambda x: x.mode()[0]  # Get the most frequent (mode) payment type\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 4., 1., 3., 2.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_reviews['review_score'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Business, Seller, and Buyer DataFrames\n",
    "\n",
    "Here, we create three main DataFrames to analyze the data from different perspectives: business, seller, and buyer. \n",
    "\n",
    "- **Business Side**: This DataFrame combines order items, orders, reviews, customer details, product information, category translations, and geolocation data. We also calculate two new columns: the average number of items per order and the net profit (which is 18% of the product price). This gives us a comprehensive view of the business operations.\n",
    "\n",
    "- **Seller Side**: This DataFrame merges order items, orders, reviews, product details, category translations, seller information, and geolocation data. We also calculate the time it takes for sellers to deliver items to the carrier and the net profit. This helps us understand the seller‚Äôs performance and logistics.\n",
    "\n",
    "- **Buyer Side**: This DataFrame focuses on the buyer‚Äôs perspective by combining order items, orders, reviews, customer details, product information, category translations, payment data, and geolocation. It includes key details like payment values and review scores, giving us insights into the buyer‚Äôs experience.\n",
    "\n",
    "Finally, we select only the relevant columns for each DataFrame to keep the data clean and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Side\n",
    "business_df = order_items.merge(orders, on=\"order_id\", how=\"left\")\n",
    "business_df = business_df.merge(order_reviews, on=\"order_id\", how=\"left\")\n",
    "business_df = business_df.merge(customers, on=\"customer_id\", how=\"left\")\n",
    "business_df = business_df.merge(products, on=\"product_id\", how=\"left\")\n",
    "business_df = business_df.merge(category_translation, on='product_category_name', how='left')\n",
    "business_df = business_df.merge(\n",
    "    geo_grouped,\n",
    "    left_on=['customer_zip_code_prefix'],\n",
    "    right_on=['geolocation_zip_code_prefix'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate new columns for Business Side\n",
    "business_df[\"avg_items_per_order\"] = business_df.groupby(\"order_id\")[\"order_item_id\"].transform(\"count\")\n",
    "business_df[\"net_profit\"] = business_df[\"price\"] * 0.18\n",
    "\n",
    "# Seller Side\n",
    "seller_df = order_items.merge(orders, on=\"order_id\", how=\"left\")\n",
    "seller_df = seller_df.merge(order_reviews, on=\"order_id\", how=\"left\")\n",
    "seller_df = seller_df.merge(products, on=\"product_id\", how=\"left\")\n",
    "seller_df = seller_df.merge(category_translation, on='product_category_name', how='left')\n",
    "seller_df = seller_df.merge(sellers, on=\"seller_id\", how=\"left\")\n",
    "seller_df = seller_df.merge(\n",
    "    geo_grouped,\n",
    "    left_on=['seller_zip_code_prefix'],\n",
    "    right_on=['geolocation_zip_code_prefix'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate new columns for Seller Side\n",
    "seller_df[\"time_deliver_to_carrier\"] = pd.to_datetime(seller_df[\"order_delivered_carrier_date\"]) - pd.to_datetime(seller_df[\"order_approved_at\"])\n",
    "seller_df[\"net_profit\"] = seller_df[\"price\"] * 0.18\n",
    "\n",
    "# Buyer Side\n",
    "buyer_df = order_items.merge(orders, on=\"order_id\", how=\"left\")\n",
    "buyer_df = buyer_df.merge(order_reviews, on=\"order_id\", how=\"left\")\n",
    "buyer_df = buyer_df.merge(customers, on=\"customer_id\", how=\"left\")\n",
    "buyer_df = buyer_df.merge(products, on=\"product_id\", how=\"left\")\n",
    "buyer_df = buyer_df.merge(category_translation, on='product_category_name', how='left')\n",
    "buyer_df = buyer_df.merge(order_payments, on=\"order_id\", how=\"left\")\n",
    "buyer_df = buyer_df.merge(\n",
    "    geo_grouped,\n",
    "    left_on=['customer_zip_code_prefix'],\n",
    "    right_on=['geolocation_zip_code_prefix'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Final Columns\n",
    "business_side = [\n",
    "    'customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_state',\n",
    "    'geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng', 'geolocation_city', 'geolocation_state',\n",
    "    'product_id', 'product_category_name_english',\n",
    "    'price',\n",
    "    'order_id', 'order_item_id', 'order_purchase_timestamp',\n",
    "    'avg_items_per_order', 'net_profit','order_status',\n",
    "]\n",
    "\n",
    "seller_side = [\n",
    "    'seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state',\n",
    "    'geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng', 'geolocation_city', 'geolocation_state',\n",
    "    'product_id', 'product_category_name_english',\n",
    "    'price', \n",
    "    'order_id', 'order_item_id', 'order_purchase_timestamp', 'order_approved_at', \n",
    "    'shipping_limit_date',\n",
    "    'order_status',\n",
    "    'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date',\n",
    "    'review_score',\n",
    "    'time_deliver_to_carrier', 'net_profit'\n",
    "]\n",
    "\n",
    "buyer_side = [\n",
    "    'customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_state',\n",
    "    'geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng', 'geolocation_city', 'geolocation_state',\n",
    "    'product_id', 'product_category_name_english',\n",
    "    'price',\n",
    "    'order_id', 'order_item_id',\n",
    "    'order_purchase_timestamp',\n",
    "    # 'payment_value',\n",
    "    'payment_type',\n",
    "    'order_status',\n",
    "    'review_score'\n",
    "]\n",
    "\n",
    "business_df = business_df[business_side]\n",
    "seller_df = seller_df[seller_side]\n",
    "buyer_df = buyer_df[buyer_side]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Data Types\n",
    "\n",
    "To ensure the data is in the right format for analysis, we convert certain columns to appropriate data types:\n",
    "\n",
    "- **Business Side**: The `order_purchase_timestamp` is converted to a datetime format, and zip code prefixes are treated as strings to avoid any numerical misinterpretation.\n",
    "\n",
    "- **Seller Side**: Similar to the business side, zip code prefixes are converted to strings. Additionally, various date-related columns (like `order_purchase_timestamp`, `order_approved_at`, etc.) are converted to datetime format. The `time_deliver_to_carrier` column, which represents the time difference between order approval and delivery to the carrier, is converted to a timedelta for easier time-based calculations.\n",
    "\n",
    "- **Buyer Side**: Zip code prefixes are converted to strings to maintain consistency across all DataFrames.\n",
    "\n",
    "These conversions are crucial for accurate data analysis and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Dtype\n",
    "# Business Side\n",
    "business_df['order_purchase_timestamp'] = pd.to_datetime(business_df['order_purchase_timestamp'])\n",
    "business_df['customer_zip_code_prefix'] = business_df['customer_zip_code_prefix'].astype(str)\n",
    "business_df['geolocation_zip_code_prefix'] = business_df['geolocation_zip_code_prefix'].astype(str)\n",
    "\n",
    "# Seller Side\n",
    "seller_df['seller_zip_code_prefix'] = seller_df['seller_zip_code_prefix'].astype(str)\n",
    "seller_df['geolocation_zip_code_prefix'] = seller_df['geolocation_zip_code_prefix'].astype(str)\n",
    "datetime_columns = [\n",
    "    'order_purchase_timestamp',\n",
    "    'order_approved_at',\n",
    "    'shipping_limit_date',\n",
    "    'order_delivered_carrier_date',\n",
    "    'order_delivered_customer_date',\n",
    "    'order_estimated_delivery_date'\n",
    "]\n",
    "\n",
    "\n",
    "for col in datetime_columns:\n",
    "    seller_df[col] = pd.to_datetime(seller_df[col])\n",
    "\n",
    "seller_df['time_deliver_to_carrier'] = pd.to_timedelta(seller_df['time_deliver_to_carrier'])\n",
    "\n",
    "#Buyer Side\n",
    "buyer_df['order_purchase_timestamp'] = pd.to_datetime(buyer_df['order_purchase_timestamp'])\n",
    "buyer_df['customer_zip_code_prefix'] = buyer_df['customer_zip_code_prefix'].astype(str)\n",
    "buyer_df['geolocation_zip_code_prefix'] = buyer_df['geolocation_zip_code_prefix'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Business DataFrame\n",
    "\n",
    "Now that we‚Äôve prepared the `business_df` DataFrame, we take a closer look at its structure using the `info()` method. This gives us a summary of the DataFrame, including the number of non-null entries, data types, and memory usage for each column. Inspecting the DataFrame not only helps us understand its structure but also ensures that the data types (dtypes) are correct. For example, we confirm that datetime columns are properly formatted and that zip codes are treated as strings. This step is essential to avoid errors in later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112650 entries, 0 to 112649\n",
      "Data columns (total 18 columns):\n",
      " #   Column                         Non-Null Count   Dtype         \n",
      "---  ------                         --------------   -----         \n",
      " 0   customer_id                    112650 non-null  object        \n",
      " 1   customer_unique_id             112650 non-null  object        \n",
      " 2   customer_zip_code_prefix       112650 non-null  object        \n",
      " 3   customer_state                 112650 non-null  object        \n",
      " 4   geolocation_zip_code_prefix    112650 non-null  object        \n",
      " 5   geolocation_lat                112650 non-null  float64       \n",
      " 6   geolocation_lng                112650 non-null  float64       \n",
      " 7   geolocation_city               112650 non-null  object        \n",
      " 8   geolocation_state              112650 non-null  object        \n",
      " 9   product_id                     112650 non-null  object        \n",
      " 10  product_category_name_english  111023 non-null  object        \n",
      " 11  price                          112650 non-null  float64       \n",
      " 12  order_id                       112650 non-null  object        \n",
      " 13  order_item_id                  112650 non-null  int64         \n",
      " 14  order_purchase_timestamp       112650 non-null  datetime64[ns]\n",
      " 15  avg_items_per_order            112650 non-null  int64         \n",
      " 16  net_profit                     112650 non-null  float64       \n",
      " 17  order_status                   112650 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(4), int64(2), object(11)\n",
      "memory usage: 15.5+ MB\n"
     ]
    }
   ],
   "source": [
    "business_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Seller DataFrame\n",
    "\n",
    "Similarly, we inspect the `seller_df` DataFrame using the `info()` method. This provides a detailed overview of the seller-side data, including the number of non-null entries, data types, and memory usage. By checking the dtypes, we ensure that all columns are in the correct format, such as datetime columns for order dates and timedelta for delivery times. This step helps us confirm that the seller-related data is clean and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112650 entries, 0 to 112649\n",
      "Data columns (total 24 columns):\n",
      " #   Column                         Non-Null Count   Dtype          \n",
      "---  ------                         --------------   -----          \n",
      " 0   seller_id                      112650 non-null  object         \n",
      " 1   seller_zip_code_prefix         112650 non-null  object         \n",
      " 2   seller_city                    112650 non-null  object         \n",
      " 3   seller_state                   112650 non-null  object         \n",
      " 4   geolocation_zip_code_prefix    112650 non-null  object         \n",
      " 5   geolocation_lat                112650 non-null  float64        \n",
      " 6   geolocation_lng                112650 non-null  float64        \n",
      " 7   geolocation_city               112650 non-null  object         \n",
      " 8   geolocation_state              112650 non-null  object         \n",
      " 9   product_id                     112650 non-null  object         \n",
      " 10  product_category_name_english  111023 non-null  object         \n",
      " 11  price                          112650 non-null  float64        \n",
      " 12  order_id                       112650 non-null  object         \n",
      " 13  order_item_id                  112650 non-null  int64          \n",
      " 14  order_purchase_timestamp       112650 non-null  datetime64[ns] \n",
      " 15  order_approved_at              112635 non-null  datetime64[ns] \n",
      " 16  shipping_limit_date            112650 non-null  datetime64[ns] \n",
      " 17  order_status                   112650 non-null  object         \n",
      " 18  order_delivered_carrier_date   111456 non-null  datetime64[ns] \n",
      " 19  order_delivered_customer_date  110196 non-null  datetime64[ns] \n",
      " 20  order_estimated_delivery_date  112650 non-null  datetime64[ns] \n",
      " 21  review_score                   111708 non-null  float64        \n",
      " 22  time_deliver_to_carrier        111441 non-null  timedelta64[ns]\n",
      " 23  net_profit                     112650 non-null  float64        \n",
      "dtypes: datetime64[ns](6), float64(5), int64(1), object(11), timedelta64[ns](1)\n",
      "memory usage: 20.6+ MB\n"
     ]
    }
   ],
   "source": [
    "seller_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Buyer DataFrame\n",
    "\n",
    "Next, we inspect the `buyer_df` DataFrame using the `info()` method. This gives us a summary of the buyer-side data, including the number of non-null entries, data types, and memory usage. By checking the dtypes, we ensure that all columns are in the correct format, such as zip codes being treated as strings and payment values as floats. This step helps us confirm that the buyer-related data is clean and ready for analysis, ensuring that the buyer‚Äôs perspective is accurately represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112650 entries, 0 to 112649\n",
      "Data columns (total 18 columns):\n",
      " #   Column                         Non-Null Count   Dtype         \n",
      "---  ------                         --------------   -----         \n",
      " 0   customer_id                    112650 non-null  object        \n",
      " 1   customer_unique_id             112650 non-null  object        \n",
      " 2   customer_zip_code_prefix       112650 non-null  object        \n",
      " 3   customer_state                 112650 non-null  object        \n",
      " 4   geolocation_zip_code_prefix    112650 non-null  object        \n",
      " 5   geolocation_lat                112650 non-null  float64       \n",
      " 6   geolocation_lng                112650 non-null  float64       \n",
      " 7   geolocation_city               112650 non-null  object        \n",
      " 8   geolocation_state              112650 non-null  object        \n",
      " 9   product_id                     112650 non-null  object        \n",
      " 10  product_category_name_english  111023 non-null  object        \n",
      " 11  price                          112650 non-null  float64       \n",
      " 12  order_id                       112650 non-null  object        \n",
      " 13  order_item_id                  112650 non-null  int64         \n",
      " 14  order_purchase_timestamp       112650 non-null  datetime64[ns]\n",
      " 15  payment_type                   112647 non-null  object        \n",
      " 16  order_status                   112650 non-null  object        \n",
      " 17  review_score                   111708 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(4), int64(1), object(12)\n",
      "memory usage: 15.5+ MB\n"
     ]
    }
   ],
   "source": [
    "buyer_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing Prices Across DataFrames\n",
    "\n",
    "To ensure consistency across all DataFrames, we calculate the total sum of the `price` column in each DataFrame: `business_df`, `seller_df`, and `buyer_df`. The total sum should be the same across all three DataFrames because they all reference the same underlying order items. This check confirms that the data merging and transformations haven‚Äôt introduced any discrepancies. In this case, the total sum matches across all DataFrames, which is a good sign that the data is consistent and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13591643.700000003"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df.price.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13591643.700000003"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seller_df.price.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13591643.700000003"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buyer_df.price.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Duplicates\n",
    "\n",
    "Before proceeding further, it‚Äôs important to ensure that our data is clean and free of duplicates. We check for duplicate rows in each DataFrame using the `duplicated()` method. The results show that there are **no duplicates** in any of the DataFrames (`business_df`, `seller_df`, or `buyer_df`). This confirms that the data is clean and ready for analysis, with no repeated rows that could skew our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seller_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buyer_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving DataFrames to CSV\n",
    "\n",
    "Once all the data processing is complete, we save the three DataFrames (`business_df`, `seller_df`, and `buyer_df`) to CSV files. These files are named `business_side.csv`, `seller_side.csv`, and `buyer_side.csv`, respectively. Saving the data to CSV files allows us to easily share or use the processed data for further analysis or reporting. This step ensures that the cleaned and transformed data is preserved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define dataset folder path (cross-platform)\n",
    "folder_path = os.path.join(\"dataset\", \"01_three_side_dataset\")\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Define file paths using os.path.join()\n",
    "business_path = os.path.join(folder_path, \"business_side.csv\")\n",
    "seller_path = os.path.join(folder_path, \"seller_side.csv\")\n",
    "buyer_path = os.path.join(folder_path, \"buyer_side.csv\")\n",
    "\n",
    "# Save DataFrames to CSV\n",
    "business_df.to_csv(business_path, index=False)\n",
    "seller_df.to_csv(seller_path, index=False)\n",
    "buyer_df.to_csv(buyer_path, index=False)\n",
    "\n",
    "print(\"CSV files saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
